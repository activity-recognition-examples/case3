{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.fftpack import fft\n",
    "from scipy.signal import find_peaks\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1338)\n",
    "cmap_data = plt.cm.Paired\n",
    "cmap_cv = plt.cm.coolwarm\n",
    "cmap_group = plt.cm.Paired\n",
    "cmap_y = plt.cm.coolwarm\n",
    "\n",
    "def visualize_groups(classes, groups):\n",
    "    # Visualize dataset groups\n",
    "    fig, ax = plt.subplots(dpi=200) \n",
    "    ax.scatter(\n",
    "        range(len(groups)),\n",
    "        [0.5] * len(groups),\n",
    "        c=groups,\n",
    "        marker=\"_\",\n",
    "        lw=50,\n",
    "        cmap=cmap_data,\n",
    "    )\n",
    "    ax.scatter(\n",
    "        range(len(groups)),\n",
    "        [3.5] * len(groups),\n",
    "        c=classes,\n",
    "        marker=\"_\",\n",
    "        lw=50,\n",
    "        cmap=cmap_data,\n",
    "    )\n",
    "    ax.set(\n",
    "        ylim=[-1, 5],\n",
    "        yticks=[0.5, 3.5],\n",
    "        yticklabels=[\"Data\\ngroup\", \"Data\\nclass\"],\n",
    "        xlabel=\"Sample index\",\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_cv_indices(cv, X, y, group, ax, n_splits, lw=10):\n",
    "    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n",
    "\n",
    "    # Generate the training/testing visualizations for each CV split\n",
    "    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y, groups=group)):\n",
    "        # Fill in indices with the training/test groups\n",
    "        indices = np.array([np.nan] * len(X))\n",
    "        indices[tt] = 1\n",
    "        indices[tr] = 0\n",
    "\n",
    "        # Visualize the results\n",
    "        ax.scatter(\n",
    "            range(len(indices)),\n",
    "            [ii + 0.5] * len(indices),\n",
    "            c=indices,\n",
    "            marker=\"_\",\n",
    "            lw=lw,\n",
    "            cmap=cmap_cv,\n",
    "            vmin=-0.2,\n",
    "            vmax=1.2,\n",
    "        )\n",
    "\n",
    "    # Plot the data classes and groups at the end\n",
    "    ax.scatter(\n",
    "        range(len(X)), [ii + 1.5] * len(X), c=y, marker=\"_\", lw=lw, cmap=cmap_data \n",
    "    )\n",
    "\n",
    "    ax.scatter(\n",
    "        range(len(X)), [ii + 2.5] * len(X), c=group, marker=\"_\", lw=lw, cmap=cmap_data \n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # Formatting\n",
    "    yticklabels = list(range(n_splits)) + [\"class\", \"group\"]\n",
    "    ax.set(\n",
    "        yticks=np.arange(n_splits + 2) + 0.5,\n",
    "        yticklabels=yticklabels,\n",
    "        xlabel=\"Sample index\",\n",
    "        ylabel=\"CV iteration\",\n",
    "        ylim=[n_splits + 2.2, -0.2],\n",
    "        xlim=[0, len(X)],\n",
    "    )\n",
    "    ax.set_title(\"{}\".format(type(cv).__name__), fontsize=15)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_segments(data, target_x, target_y, target_group,  window_size=80, step_size=40):\n",
    "    segments = []\n",
    "    labels = []\n",
    "    group = []\n",
    "\n",
    "    for i in range(0, len(data) - window_size, step_size):\n",
    "        segments_data = []\n",
    "        for x in target_x:\n",
    "            x_values = data[x].values[i:i + window_size]\n",
    "            segments_data.append(x_values)\n",
    "        label = data[target_y].values[i]\n",
    "        member = data[target_group].values[i]\n",
    "\n",
    "        segments.append(segments_data)\n",
    "        labels.append(label)\n",
    "        group.append(member)\n",
    "    return segments, labels, group\n",
    "\n",
    "\n",
    "def create_label_segments(data, target, window_size=80, step_size=40):\n",
    "    ys = []\n",
    "    for i in range(0, len(data) - window_size, step_size):\n",
    "        segments_data = []\n",
    "        y = data[target].values[i]\n",
    "        y.append(y)\n",
    "    return ys\n",
    "\n",
    "\n",
    "def extract_time_features(segments):\n",
    "    features = []\n",
    "    for segment in segments:\n",
    "        segment_features = []\n",
    "        for axis in segment:\n",
    "            mean_axis = np.mean(axis)\n",
    "            std_axis = np.std(axis)\n",
    "            segment_features.extend([mean_axis, std_axis])\n",
    "        features.append(segment_features)\n",
    "    return np.array(features)\n",
    "\n",
    "def extract_frequency_features(segments, sampling_rate=100):\n",
    "    features = []\n",
    "    for segment in segments:\n",
    "        segment_features = []\n",
    "        for axis in segment:\n",
    "            fft_axis = np.abs(fft(axis))\n",
    "\n",
    "            # Peak frequency and maximum amplitude\n",
    "            peak_indices, _ = find_peaks(fft_axis)\n",
    "\n",
    "            if len(peak_indices) > 0:\n",
    "                peak_freq = peak_indices[np.argmax(fft_axis[peak_indices])] / len(axis) * sampling_rate\n",
    "                max_amplitude = np.max(fft_axis[peak_indices])\n",
    "            else:\n",
    "                peak_freq = 0.0  # デフォルトの値を設定\n",
    "                max_amplitude = 0.0  # デフォルトの値を設定\n",
    "\n",
    "            # Signal energy\n",
    "            energy = np.sum(axis ** 2) / len(axis)\n",
    "\n",
    "            segment_features.extend([peak_freq, max_amplitude, energy])\n",
    "\n",
    "        features.append(segment_features)\n",
    "\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('03_exercise_toy_dataset.csv', index_col=0)\n",
    "\n",
    "# 欠損値の除去\n",
    "data.dropna(axis=0, how=\"any\", inplace=True)\n",
    "\n",
    "# セグメントを用意し、時間領域の特徴量と周波数領域の特徴量を抽出\n",
    "window_size = 80\n",
    "step_size = 40\n",
    "target_x = ['left-wrist_Accs.X','left-wrist_Accs.Y', 'left-wrist_Accs.Z']\n",
    "target_y = \"action\" \n",
    "terget_group = \"user_id\"\n",
    "target_names = list(data[target_y].unique())\n",
    "\n",
    "# ラベルエンコーダーのインスタンス化\n",
    "le0 = LabelEncoder()\n",
    "le1 = LabelEncoder()\n",
    "data[\"action\"] = le0.fit_transform(data[\"action\"])\n",
    "data[\"user_id\"] = le1.fit_transform(data[\"user_id\"])\n",
    "\n",
    "segments, labels, groups = create_segments(data, target_x, target_y, terget_group, window_size, step_size)\n",
    "\n",
    "# 特徴量抽出\n",
    "time_features = extract_time_features(segments)\n",
    "frequency_features = extract_frequency_features(segments)\n",
    "\n",
    "# 時間領域の特徴量と周波数領域の特徴量を統合\n",
    "combined_features = np.concatenate((time_features, frequency_features), axis=1)\n",
    "\n",
    "# データセットの分割\n",
    "X = combined_features\n",
    "y = np.asarray(labels, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ランダムフォレスト分類器をインスタンス化\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8216666666666667\n",
      "0.8933333333333333\n",
      "0.5416666666666666\n",
      "0.7316666666666667\n",
      "0.9141666666666667\n",
      "0.8408333333333333\n",
      "0.9141666666666667\n",
      "0.7816666666666666\n",
      "0.7275\n",
      "0.7404006677796328\n"
     ]
    }
   ],
   "source": [
    "# LeaveOneGroupOutのインスタンスを作成\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "all_y_true = []\n",
    "all_y_pred = []\n",
    "\n",
    "# 分割数、ここでは5分割\n",
    "for train_index, test_index in logo.split(X, y, groups):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "\n",
    "    # モデルの学習\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # 予測\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # 保存\n",
    "    all_y_true.extend(y_test)\n",
    "    all_y_pred.extend(y_pred)\n",
    "    \n",
    "    # 予測結果の評価（ここでは正解率）\n",
    "    print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "all_y_true = le0.inverse_transform(np.array(all_y_true).astype(np.int32))\n",
    "all_y_pred = le0.inverse_transform(np.array(all_y_pred).astype(np.int32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[1107    9  144    0  159   60    7   14]\n",
      " [   9 1398   46    0    0    0    4   43]\n",
      " [ 100   10 1047    3  245   94    1    0]\n",
      " [   2    0    2 1045    0    2  449    0]\n",
      " [  70    1  187    0 1175   65    0    2]\n",
      " [ 159    0  171    3   59 1108    0    0]\n",
      " [   1   25    0  280    0    0 1194    0]\n",
      " [  43   34    6    1    1    0    0 1413]]\n",
      "\n",
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      airchair       0.74      0.74      0.74      1500\n",
      "        crunch       0.95      0.93      0.94      1500\n",
      "          dips       0.65      0.70      0.67      1500\n",
      "         plank       0.78      0.70      0.74      1500\n",
      "        pushup       0.72      0.78      0.75      1500\n",
      "  reverseplank       0.83      0.74      0.78      1500\n",
      "sideplank-left       0.72      0.80      0.76      1500\n",
      "         squat       0.96      0.94      0.95      1498\n",
      "\n",
      "      accuracy                           0.79     11998\n",
      "     macro avg       0.79      0.79      0.79     11998\n",
      "  weighted avg       0.79      0.79      0.79     11998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 全ユーザの結果に基づく混同行列と分類レポートを表示\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(all_y_true, all_y_pred, labels=target_names))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_y_true, all_y_pred, labels=target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
